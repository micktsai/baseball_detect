import os
import shutil
import glob
import random
import cv2
import yaml
import albumentations as A
from roboflow import Roboflow
from ultralytics import YOLO
import torch
from dotenv import load_dotenv
from tqdm import tqdm

# Load environment variables
load_dotenv()

# ===============================
# [æ ¸å¿ƒ] è£åˆ‡è³‡æ–™é›†è£½ä½œå‡½æ•¸
# ===============================
def create_crop_dataset(source_path, output_path, crop_size=640, jitter_ratio=0.2, negative_ratio=0.1):
    """
    è®€å–åŸå§‹é«˜è§£æåº¦è³‡æ–™é›†ï¼Œç”Ÿæˆä»¥çƒç‚ºä¸­å¿ƒçš„å±€éƒ¨è£åˆ‡è³‡æ–™é›†
    """
    if os.path.exists(output_path):
        shutil.rmtree(output_path)
    
    splits = ['train', 'valid', 'test']
    
    # å»ºç«‹ç›®éŒ„çµæ§‹
    for split in splits:
        os.makedirs(os.path.join(output_path, split, 'images'), exist_ok=True)
        os.makedirs(os.path.join(output_path, split, 'labels'), exist_ok=True)

    print(f"âœ‚ï¸  é–‹å§‹è£½ä½œè£åˆ‡è³‡æ–™é›†...")
    print(f"    ä¾†æº: {source_path}")
    print(f"    ç›®æ¨™: {output_path}")
    print(f"    è¦æ ¼: {crop_size}x{crop_size} (Jitter: {jitter_ratio}, Neg: {negative_ratio})")

    total_crops = 0

    for split in splits:
        img_dir = os.path.join(source_path, split, 'images')
        lbl_dir = os.path.join(source_path, split, 'labels')
        
        if not os.path.exists(img_dir): continue

        img_paths = glob.glob(os.path.join(img_dir, "*"))
        print(f"    æ­£åœ¨è™•ç† {split} ({len(img_paths)} å¼µåŸåœ–)...")

        for img_path in tqdm(img_paths):
            filename = os.path.basename(img_path)
            name, ext = os.path.splitext(filename)
            lbl_path = os.path.join(lbl_dir, f"{name}.txt")

            image = cv2.imread(img_path)
            if image is None: continue
            h_img, w_img, _ = image.shape

            boxes = [] 
            if os.path.exists(lbl_path):
                with open(lbl_path, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            boxes.append([int(parts[0])] + [float(x) for x in parts[1:]])

            # --- 1. æ­£æ¨£æœ¬ (æœ‰çƒ) ---
            for i, box in enumerate(boxes):
                cls, xc, yc, bw, bh = box
                abs_xc, abs_yc = xc * w_img, yc * h_img
                
                # éš¨æ©Ÿåç§» (æ¨¡æ“¬è¿½è¹¤èª¤å·®)
                offset_limit = crop_size * jitter_ratio
                off_x = random.uniform(-offset_limit, offset_limit)
                off_y = random.uniform(-offset_limit, offset_limit)
                
                crop_cx = abs_xc + off_x
                crop_cy = abs_yc + off_y
                
                x1 = int(crop_cx - crop_size / 2)
                y1 = int(crop_cy - crop_size / 2)
                
                # é‚Šç•Œé™åˆ¶
                x1 = max(0, min(x1, w_img - crop_size))
                y1 = max(0, min(y1, h_img - crop_size))
                x2, y2 = x1 + crop_size, y1 + crop_size

                crop_img = image[y1:y2, x1:x2]
                
                # è½‰æ› Label
                new_labels = []
                for b in boxes:
                    b_cls, b_xc, b_yc, b_bw, b_bh = b
                    b_abs_x, b_abs_y = b_xc * w_img, b_yc * h_img
                    b_abs_w, b_abs_h = b_bw * w_img, b_bh * h_img
                    
                    if x1 < b_abs_x < x2 and y1 < b_abs_y < y2:
                        n_xc = (b_abs_x - x1) / crop_size
                        n_yc = (b_abs_y - y1) / crop_size
                        n_bw = b_abs_w / crop_size
                        n_bh = b_abs_h / crop_size
                        
                        # Clip 0-1
                        n_xc = max(0, min(1, n_xc))
                        n_yc = max(0, min(1, n_yc))
                        n_bw = max(0, min(1, n_bw))
                        n_bh = max(0, min(1, n_bh))
                        
                        new_labels.append(f"{b_cls} {n_xc:.6f} {n_yc:.6f} {n_bw:.6f} {n_bh:.6f}")

                if new_labels:
                    s_name = f"{name}_c{i}"
                    cv2.imwrite(os.path.join(output_path, split, 'images', f"{s_name}.jpg"), crop_img)
                    with open(os.path.join(output_path, split, 'labels', f"{s_name}.txt"), 'w') as f:
                        f.write("\n".join(new_labels))
                    total_crops += 1

            # --- 2. è² æ¨£æœ¬ (éš¨æ©ŸèƒŒæ™¯) ---
            if random.random() < negative_ratio:
                rx = random.randint(0, max(1, w_img - crop_size))
                ry = random.randint(0, max(1, h_img - crop_size))
                
                # æª¢æŸ¥æ˜¯å¦æœ‰çƒ
                has_ball = False
                for b in boxes:
                    bx, by = b[1] * w_img, b[2] * h_img
                    if rx < bx < rx + crop_size and ry < by < ry + crop_size:
                        has_ball = True; break
                
                if not has_ball:
                    bg_crop = image[ry:ry+crop_size, rx:rx+crop_size]
                    bg_name = f"{name}_bg"
                    cv2.imwrite(os.path.join(output_path, split, 'images', f"{bg_name}.jpg"), bg_crop)
                    open(os.path.join(output_path, split, 'labels', f"{bg_name}.txt"), 'w').close()
                    total_crops += 1
    
    print(f"âœ… è£åˆ‡å®Œæˆï¼å…±ç”Ÿæˆ {total_crops} å¼µåœ–ç‰‡ã€‚")

    # å»ºç«‹ data.yaml
    yaml_content = {
        'path': os.path.abspath(output_path),
        'train': 'train/images',
        'val': 'valid/images',
        'test': 'test/images',
        'nc': 1,
        'names': ['baseball']
    }
    with open(os.path.join(output_path, 'data.yaml'), 'w') as f:
        yaml.dump(yaml_content, f)


# ===============================
# å½±åƒå¢å¼· (é‡å°è£åˆ‡å¾Œçš„åœ–ç‰‡)
# ===============================
def augment_dataset(dataset_location, augment_ratio=0.35):
    """
    å°è£åˆ‡å¾Œçš„ train/images åš motion blur
    """
    print(f"ğŸ”„ é–‹å§‹ Augmentation (Target: {dataset_location}, Ratio={augment_ratio})")

    img_dir = os.path.join(dataset_location, "train", "images")
    lbl_dir = os.path.join(dataset_location, "train", "labels")

    transform = A.Compose([
        A.OneOf([
            A.MotionBlur(blur_limit=(3, 5), p=1), # å°æ¨¡ç³Š
            A.MotionBlur(blur_limit=(5, 7), p=1), # ä¸­æ¨¡ç³Š
        ], p=0.7),
        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.2),
        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.15, p=0.3),
    ])

    image_paths = glob.glob(os.path.join(img_dir, "*.jpg"))
    
    count = 0
    for img_path in image_paths:
        if random.random() > augment_ratio: continue

        image = cv2.imread(img_path)
        if image is None: continue

        augmented = transform(image=image)["image"]
        
        name, ext = os.path.splitext(os.path.basename(img_path))
        new_path = os.path.join(img_dir, f"{name}_mb{ext}")
        cv2.imwrite(new_path, augmented)

        # è¤‡è£½ Label
        old_lbl = os.path.join(lbl_dir, f"{name}.txt")
        new_lbl = os.path.join(lbl_dir, f"{name}_mb.txt")
        if os.path.exists(old_lbl):
            shutil.copy(old_lbl, new_lbl)
            count += 1

    print(f"âœ… Augmentation å®Œæˆï¼Œæ–°å¢ {count} å¼µæ¨¡ç³Šæ¨£æœ¬")


# ===============================
# ä¸»æµç¨‹
# ===============================
def main():
    torch.cuda.empty_cache()
    
    # è¨­å®šåç¨± (è¨˜å¾—åŠ ä¸Š crop æ¨™è¨˜ä»¥ç¤ºå€åˆ¥)
    name = "yolov8n_p2_crop640_v1"
    
    # ===============================
    # 1. Roboflow ä¸‹è¼‰ (Raw Data)
    # ===============================
    rf = Roboflow(api_key=os.getenv("ROBOFLOW_API_KEY")) 
    project = rf.workspace("mickshelbytsai").project("pitch-tracking-sgse6")
    
    # âš ï¸ è«‹å‹™å¿…åœ¨æ­¤ä¿®æ”¹ç‚ºä½ çš„ "No Resize" Version ç‰ˆæœ¬è™Ÿ
    version = project.version(3) 
    print("â¬‡ï¸ ä¸‹è¼‰åŸå§‹è³‡æ–™é›†...")
    dataset = version.download("yolov8")
    
    # ===============================
    # 2. åŸ·è¡Œè£åˆ‡ (Raw -> Crop)
    # ===============================
    crop_dataset_dir = "dataset_cropped_640"
    create_crop_dataset(
        source_path=dataset.location, 
        output_path=crop_dataset_dir, 
        crop_size=640,      # é€™è£¡è¨­å®š 640 ä»¥ç¬¦åˆè¨“ç·´
        jitter_ratio=0.2,   # å…è¨±ä¸­å¿ƒé»åç§» 20%
        negative_ratio=0.1  # 10% èƒŒæ™¯åœ–
    )

    # ===============================
    # 3. åŸ·è¡Œå¢å¼· (On Cropped Data)
    # ===============================
    augment_dataset(crop_dataset_dir, augment_ratio=0.3)

    # ===============================
    # 4. å»ºç«‹èˆ‡è¨“ç·´æ¨¡å‹
    # ===============================
    # æ—¢ç„¶çƒè®Šå¤§äº†ï¼Œp2 æ¶æ§‹ä¾ç„¶å¯ä»¥ç”¨ï¼Œä½†æ•ˆæœæœƒæ›´é¡¯è‘—
    model = YOLO("./yolov8n-p2-new.yaml") 
    
    model.train(
        data=os.path.join(crop_dataset_dir, "data.yaml"), # æŒ‡å‘è£åˆ‡å¾Œçš„è³‡æ–™
        epochs=150,          
        patience=30,         # çµ¦ä»–å¤šä¸€é»è€å¿ƒ
        imgsz=640,           # â­ï¸ é—œéµï¼šé™å› 640ï¼Œå› ç‚ºé€™æ˜¯è£åˆ‡åœ–
        batch=16,            # â­ï¸ é—œéµï¼šåœ–è®Šå°äº†ï¼ŒBatch é–‹å¤§ï¼(è©¦è©¦ 16 æˆ– 32)
        workers=0,
        amp=True,
        name=name,
        exist_ok=True,
        mosaic=0.0,          
    )

if __name__ == "__main__":
    main()