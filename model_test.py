import cv2
import numpy as np
try:
    import tflite_runtime.interpreter as tflite
except ImportError:
    try:
        import tensorflow.lite as tflite
    except ImportError:
        print("Error: neither 'tflite_runtime' nor 'tensorflow' is installed.")
        print("Please install one of them: pip install tflite-runtime OR pip install tensorflow")
        exit(1)
import os
import argparse
import glob
from ultralytics import YOLO
from tqdm import tqdm

# ==========================================
# åƒæ•¸è¨­å®š
# ==========================================
MODEL_PATH = "models/yolo8n_p2new/yolov8n_p2new_mblur_40_db_saved_model/yolov8n_p2new_mblur_40_db_float16.tflite"
OUTPUT_BASE_DIR = "results"

ROI_WIDTH = 480
ROI_HEIGHT = 320

# é¡è‰²å®šç¾© (BGR)
ROI_COLOR = (0, 255, 0)    # Green
BOX_COLOR = (0, 0, 255)    # Red

# ç‹€æ…‹å®šç¾©
STATE_SEARCH = 0
STATE_TRACK = 1

def get_video_path():
    """è§£æ CLI åƒæ•¸ä¸¦å›å‚³å½±ç‰‡è·¯å¾‘"""
    parser = argparse.ArgumentParser(description="Baseball Ball Detection & Tracking")
    parser.add_argument("-v", "--video", type=str, help="Video filename (in videos/ folder), e.g., 'monster1'")
    args = parser.parse_args()
    
    default_video = "videos/monster2.MP4"
    
    if args.video:
        # 1. æª¢æŸ¥æ˜¯å¦ç›´æ¥æ˜¯è·¯å¾‘
        if os.path.exists(args.video):
            return args.video
            
        # 2. æª¢æŸ¥ videos/ ä¸‹çš„å®Œæ•´æª”å
        path_in_videos = os.path.join("videos", args.video)
        if os.path.exists(path_in_videos):
            return path_in_videos
            
        # 3. å˜—è©¦æœå°‹ videos/ ä¸‹çš„åŒåæª”æ¡ˆ (å¿½ç•¥å‰¯æª”å)
        candidates = glob.glob(os.path.join("videos", f"{args.video}.*"))
        if candidates:
            # å„ªå…ˆæ‰¾ mp4, MP4, mov, MOV
            print(f"ğŸ” Found candidates: {candidates}")
            return candidates[0]
            
        print(f"âŒ Error: Video '{args.video}' not found in videos/ directory.")
        exit(1)
    else:
        return default_video

def load_models():
    """è¼‰å…¥ YOLO-Pose å’Œ TFLite æ¨¡å‹"""
    # åˆå§‹åŒ– Pose æ¨¡å‹ (YOLOv8-Pose)
    print("ğŸš€ æ­£åœ¨è¼‰å…¥ Pose æ¨¡å‹: yolov8n-pose.pt")
    pose_model = YOLO('yolov8n-pose.pt')
    
    # è¼‰å…¥ TFLite æ¨¡å‹
    print(f"ğŸš€ æ­£åœ¨è¼‰å…¥ Ball Detection æ¨¡å‹: {MODEL_PATH}")
    interpreter = tflite.Interpreter(model_path=MODEL_PATH)
    interpreter.allocate_tensors()
    return pose_model, interpreter

def get_roi(center_x, center_y, frame_width, frame_height):
    """è¨ˆç®— ROI åº§æ¨™ï¼ŒåŒ…å«äº†é‚Šç•Œæª¢æŸ¥"""
    x1 = max(0, center_x - ROI_WIDTH // 2)
    y1 = max(0, center_y - ROI_HEIGHT // 2)
    x2 = min(frame_width, center_x + ROI_WIDTH // 2)
    y2 = min(frame_height, center_y + ROI_HEIGHT // 2)
    
    # ç¢ºä¿ ROI å¤§å°å›ºå®š (é™¤äº†é‚Šç•Œ)
    if x2 - x1 < ROI_WIDTH:
        if x1 == 0:
            x2 = min(frame_width, x1 + ROI_WIDTH)
        else:
            x1 = max(0, x2 - ROI_WIDTH)
            
    if y2 - y1 < ROI_HEIGHT:
        if y1 == 0:
            y2 = min(frame_height, y1 + ROI_HEIGHT)
        else:
            y1 = max(0, y2 - ROI_HEIGHT)
            
    return int(x1), int(y1), int(x2), int(y2)

def analyze_pose(frame, pose_model):
    """åŸ·è¡Œéª¨æ¶åˆ†æï¼Œå›å‚³å³æ‰‹ä½ç½® (x, y) æˆ– (-1, -1)"""
    pose_results = pose_model(frame, verbose=False)
    
    hand_x, hand_y = -1, -1
    
    if pose_results and len(pose_results[0].keypoints) > 0:
        keypoints = pose_results[0].keypoints
        if keypoints is not None and keypoints.conf is not None:
             # COCO Keypoint Index 10 is Right Wrist
            rw_idx = 10
            
            # å–å¾— Right Wrist è³‡æ–™
            if keypoints.xy.shape[1] > rw_idx:
                rw_x = keypoints.xy[0][rw_idx][0].item()
                rw_y = keypoints.xy[0][rw_idx][1].item()
                rw_conf = keypoints.conf[0][rw_idx].item()
                
                if rw_conf > 0.5:
                    hand_x, hand_y = int(rw_x), int(rw_y)
    
    return hand_x, hand_y

def run_tflite_inference(frame_roi, interpreter):
    """åœ¨ ROI ä¸ŠåŸ·è¡Œ TFLite æ¨¡å‹æ¨è«–"""
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    input_shape = input_details[0]['shape'] # [1, 640, 640, 3]

    input_h, input_w = input_shape[1], input_shape[2]
    
    # é è™•ç†
    img = cv2.resize(frame_roi, (input_w, input_h))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.float32) / 255.0
    img = np.expand_dims(img, axis=0)
    
    # æ¨è«–
    interpreter.set_tensor(input_details[0]['index'], img)
    interpreter.invoke()
    
    # è§£æè¼¸å‡º (å‡è¨­è¼¸å‡ºæ ¼å¼ [1, 5, 8400])
    output_data = interpreter.get_tensor(output_details[0]['index'])[0] 
    output_data = output_data.T
    
    boxes = []
    scores = []
    
    # é–¾å€¼éæ¿¾
    conf_threshold = 0.45 
    
    for i in range(len(output_data)):
        row = output_data[i]
        score = row[4] # class score (ball)
        
        if score > conf_threshold:
            # YOLO output: cx, cy, w, h (normalized relative to 640x640)
            cx, cy, w, h = row[0], row[1], row[2], row[3]
            
            # è½‰å› ROI åº§æ¨™ (480x320)
            # å› ç‚ºæˆ‘å€‘ resize æˆ 640x640 ä¸Ÿé€²å»ï¼Œæ‰€ä»¥è¦é‚„åŸæ¯”ä¾‹
            scale_x = frame_roi.shape[1] / input_w
            scale_y = frame_roi.shape[0] / input_h
            
            x1 = int((cx - w/2) * input_w * scale_x)
            y1 = int((cy - h/2) * input_h * scale_y)
            x2 = int((cx + w/2) * input_w * scale_x)
            y2 = int((cy + h/2) * input_h * scale_y)
            
            boxes.append([x1, y1, x2, y2])
            scores.append(float(score))
            
    return boxes, scores

def process_video(video_path):
    """ä¸»å½±ç‰‡è™•ç†æµç¨‹"""
    pose_model, interpreter = load_models()
    
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # æº–å‚™è¼¸å‡ºç›®éŒ„
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    model_name = "yolov8n_p2new_mblur_40_db_float16" # ç°¡åŒ–åç¨±
    result_dir = os.path.join(OUTPUT_BASE_DIR, model_name, video_name)
    os.makedirs(result_dir, exist_ok=True)
    
    # æ¸…ç©ºèˆŠçµæœ
    for f in os.listdir(result_dir):
        os.remove(os.path.join(result_dir, f))
        
    print(f"ğŸï¸ é–‹å§‹è™•ç†å½±ç‰‡ï¼Œå…± {total_frames} å¹€ ({frame_width}x{frame_height})...")
    
    frame_idx = 1
    pbar = tqdm(total=total_frames, desc="Processing Frames")
    
    # ç‹€æ…‹è®Šæ•¸
    current_state = STATE_SEARCH
    
    last_ball_x = 0
    last_ball_y = 0
    roi_direction = 0 # 1: right, -1: left
    
    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            break

        roi_x1, roi_y1, roi_x2, roi_y2 = 0, 0, 0, 0
        current_roi_frame = None
        ball_detected = False
        
        annotated_frame = frame.copy()

        # ---------------------------------------------------------
        # 1. ç‹€æ…‹æ©Ÿï¼šSEARCH æ¨¡å¼
        # ---------------------------------------------------------
        if current_state == STATE_SEARCH:
            hand_center_x, hand_center_y = analyze_pose(frame, pose_model)
            
            if hand_center_x != -1:
                # è¦–è¦ºåŒ–å³æ‰‹
                cv2.circle(annotated_frame, (hand_center_x, hand_center_y), 8, (0, 255, 255), -1)
                cv2.putText(annotated_frame, "Right Hand", (hand_center_x + 10, hand_center_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
                
                # æ±ºå®šåˆå§‹æ–¹å‘ï¼šæ‰‹åœ¨çƒçš„å·¦é‚Š(ç•«é¢å·¦å´) -> ROI å¾€å³èµ°(é é›¢æ‰‹)
                if hand_center_x < frame_width / 2:
                    roi_direction = 1 # å¾€å³ (é é›¢æ‰‹)
                else:
                    roi_direction = -1 # å¾€å·¦ (é é›¢æ‰‹)
                
                # æŠ“å–å³æ‰‹é™„è¿‘çš„ ROI
                roi_x1, roi_y1, roi_x2, roi_y2 = get_roi(hand_center_x, hand_center_y, frame_width, frame_height)
                current_roi_frame = frame[roi_y1:roi_y2, roi_x1:roi_x2]
                
                # ç•«å‡ºæœå°‹ ROI
                cv2.rectangle(annotated_frame, (roi_x1, roi_y1), (roi_x2, roi_y2), ROI_COLOR, 2)
                cv2.putText(annotated_frame, "Searching Ball (Right Hand)", (roi_x1, roi_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, ROI_COLOR, 2)

        # ---------------------------------------------------------
        # 2. ç‹€æ…‹æ©Ÿï¼šTRACK æ¨¡å¼
        # ---------------------------------------------------------
        elif current_state == STATE_TRACK:
            # è¿½è¹¤æ¨¡å¼ä¸‹ï¼ŒROI æ ¹æ“šçƒçš„æœ€å¾Œå’Œæ–¹å‘å¤–æ¨
            # ç­–ç•¥ï¼šä»¥çƒç‚ºä¸­å¿ƒï¼Œå¾€ã€Œé é›¢æ‰‹ã€çš„æ–¹å‘æ¨ 50 px (Leading ROI)
            roi_center_x = last_ball_x + (50 * roi_direction)
            roi_center_y = last_ball_y 
            
            roi_x1, roi_y1, roi_x2, roi_y2 = get_roi(roi_center_x, roi_center_y, frame_width, frame_height)
            current_roi_frame = frame[roi_y1:roi_y2, roi_x1:roi_x2]
            
            # ç•«å‡ºè¿½è¹¤ ROI
            cv2.rectangle(annotated_frame, (roi_x1, roi_y1), (roi_x2, roi_y2), ROI_COLOR, 2)
            cv2.putText(annotated_frame, "Tracking Ball (Sticky)", (roi_x1, roi_y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, ROI_COLOR, 2)

        # ---------------------------------------------------------
        # 3. åŸ·è¡Œçƒé«”åµæ¸¬ (TFLite)
        # ---------------------------------------------------------
        if current_roi_frame is not None and current_roi_frame.size > 0:
            boxes, scores = run_tflite_inference(current_roi_frame, interpreter)
            
            best_score = 0
            best_box = None
            
            if len(boxes) > 0:
                # æ‰¾å‡ºæœ€é«˜åˆ†çš„
                idx = np.argmax(scores)
                best_score = scores[idx]
                box = boxes[idx]
                
                # è½‰æ›å›å…¨åŸŸåº§æ¨™
                x1, y1, x2, y2 = box
                abs_x1 = x1 + roi_x1
                abs_y1 = y1 + roi_y1
                abs_x2 = x2 + roi_x1
                abs_y2 = y2 + roi_y1
                
                best_box = [abs_x1, abs_y1, abs_x2, abs_y2]
                
                # ç•«å‡ºçƒ
                label = f"ball {best_score:.2f}"
                cv2.rectangle(annotated_frame, (abs_x1, abs_y1), (abs_x2, abs_y2), BOX_COLOR, 2)
                cv2.putText(annotated_frame, label, (abs_x1, abs_y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

            # ---------------------------------------------------------
            # 4. æ›´æ–°ç‹€æ…‹
            # ---------------------------------------------------------
            if best_box:
                # åµæ¸¬åˆ°çƒ
                ball_detected = True
                bx1, by1, bx2, by2 = best_box
                current_ball_x = (bx1 + bx2) / 2
                current_ball_y = (by1 + by2) / 2
                
                last_ball_x = current_ball_x
                last_ball_y = current_ball_y
                
                # å¦‚æœæ˜¯å¾ Search è½‰ Trackï¼Œæ±ºå®šè¿½è¹¤æ–¹å‘ä¸¦æª¢æŸ¥æ‰‹ä¸­çƒ
                if current_state == STATE_SEARCH:
                    # è¨ˆç®—çƒèˆ‡æ‰‹çš„è·é›¢ (éœ€è¦ hand_center_x, hand_center_y from prev step)
                    # æ³¨æ„ï¼šåœ¨ SEARCH æ¨¡å¼ä¸‹ hand_center_x æ˜¯æœ‰å®šç¾©çš„
                    dist_to_hand = ((current_ball_x - hand_center_x)**2 + (current_ball_y - hand_center_y)**2)**0.5
                    
                    if dist_to_hand < 15: # User suggested 15px logic check from edit, previously was 5px
                         # çƒé›¢æ‰‹å¤ªè¿‘ï¼Œè¦–ç‚ºé‚„åœ¨æ‰‹ä¸­
                        print(f"Frame {frame_idx}: Ball close to hand ({dist_to_hand:.1f}px). Staying in SEARCH.")
                        # ä¿æŒä¸è®Šï¼Œç¹¼çºŒæœå°‹
                    else:
                        # è·é›¢å¤ é ï¼Œè¦–ç‚ºæŠ•å‡º
                        if current_ball_x > hand_center_x:
                            roi_direction = 1 # å¾€å³
                        else:
                            roi_direction = -1 # å¾€å·¦
                        
                        print(f"Frame {frame_idx}: Released! Dist: {dist_to_hand:.1f}, Dir: {roi_direction}")
                        current_state = STATE_TRACK
                
                else:
                    # å·²ç¶“åœ¨ Track æ¨¡å¼ï¼Œå°±ç¹¼çºŒ Track
                    current_state = STATE_TRACK
            
            else:
                # è©² ROI æ²’æ‰¾åˆ°çƒ (å¯èƒ½æ˜¯è·Ÿä¸Ÿæˆ–è¢«é®æ“‹)
                if current_state == STATE_TRACK:
                    # å›ºå®šå¤–æ¨ (Fixed Extrapolation)
                    last_ball_x += (100 * roi_direction)
                    # ä¿æŒ Track
                    current_state = STATE_TRACK
                else:
                    # Search æ¨¡å¼æ²’æ‰¾åˆ° -> ç¹¼çºŒ Search
                    current_state = STATE_SEARCH

        # å„²å­˜åœ–ç‰‡
        output_filename = f"img_result{frame_idx}.jpg"
        output_path = os.path.join(result_dir, output_filename)
        cv2.imwrite(output_path, annotated_frame)

        frame_idx += 1
        pbar.update(1)

    cap.release()
    pbar.close()
    print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼æ‰€æœ‰çµæœå·²å­˜è‡³ {result_dir}")

if __name__ == "__main__":
    video_path = get_video_path()
    process_video(video_path)
